{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpheus : MVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mediapipe matplotlib numpy opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download required models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Imports modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Helpers and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a shortcut\n",
    "DrawingSpec = solutions.drawing_utils.DrawingSpec\n",
    "\n",
    "# Folder where captured images will be saved\n",
    "SAVE_DIR = './images/'\n",
    "\n",
    "# Define number of pixels on the saved images\n",
    "SAVED_IMAGE_PIXELS = 256\n",
    "\n",
    "# Define indices of landmarks we should pay attention\n",
    "# NOTE: Sorted: [superior_mouth, inferior_mouth, chin, right_ear, left_ear, right_jaw, left_jaw]\n",
    "REQUIRED_LANDMARKS = [0, 17, 152, 234, 454, 172, 397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function which zoom  and scale image\n",
    "def zoom(img, zoom_factor=2):\n",
    "    return cv2.resize(img, None, fx=zoom_factor, fy=zoom_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that plot each landmarks detected\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title(\"Face Blendshapes\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Core function which draws landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    # Draw landmarks on the face if there are in the REQUIRED_LANDMARKS array\n",
    "    for l, landmark in enumerate(face_landmarks):\n",
    "      if l in REQUIRED_LANDMARKS:\n",
    "        face_landmarks_proto.landmark.extend([landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z)])\n",
    "\n",
    "    # Just a quick demo to show its possible to measure distance between landmarks\n",
    "    x1, y1, z1 = face_landmarks[0].x, face_landmarks[0].y, face_landmarks[0].z\n",
    "    x2, y2, z2 = face_landmarks[17].x, face_landmarks[17].y, face_landmarks[17].z\n",
    "    obm = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2 + (z2 - z1) ** 2)\n",
    "    # print(obm)\n",
    "\n",
    "    # Define a drawing style for the landmarks\n",
    "    landmarks_drawing_spec = (DrawingSpec(color=(255, 0, 0), thickness=5, circle_radius=2))\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=None,\n",
    "        landmark_drawing_spec=landmarks_drawing_spec,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=None,\n",
    "        landmark_drawing_spec=landmarks_drawing_spec,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=None,\n",
    "          landmark_drawing_spec=landmarks_drawing_spec,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create an FaceLandmarker object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Load the input image (webcam) and draw landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__(): incompatible constructor arguments. The following argument types are supported:\n    1. mediapipe.python._framework_bindings.image.Image(image_format: mediapipe::ImageFormat_Format, data: numpy.ndarray[numpy.uint8])\n    2. mediapipe.python._framework_bindings.image.Image(image_format: mediapipe::ImageFormat_Format, data: numpy.ndarray[numpy.uint16])\n    3. mediapipe.python._framework_bindings.image.Image(image_format: mediapipe::ImageFormat_Format, data: numpy.ndarray[numpy.float32])\n\nInvoked with: kwargs: image_format=<ImageFormat.SRGB: 1>, data=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chaker\\Desktop\\learning\\Non Alternant\\ProjetSimilaire\\quantane test\\Morpheus-research\\research\\MVP.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chaker/Desktop/learning/Non%20Alternant/ProjetSimilaire/quantane%20test/Morpheus-research/research/MVP.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m success, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chaker/Desktop/learning/Non%20Alternant/ProjetSimilaire/quantane%20test/Morpheus-research/research/MVP.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Convert it to mediapipe format\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chaker/Desktop/learning/Non%20Alternant/ProjetSimilaire/quantane%20test/Morpheus-research/research/MVP.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39;49mImage(image_format\u001b[39m=\u001b[39;49mmp\u001b[39m.\u001b[39;49mImageFormat\u001b[39m.\u001b[39;49mSRGB, data\u001b[39m=\u001b[39;49mimg)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaker/Desktop/learning/Non%20Alternant/ProjetSimilaire/quantane%20test/Morpheus-research/research/MVP.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Detect face landmarks from the input image.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaker/Desktop/learning/Non%20Alternant/ProjetSimilaire/quantane%20test/Morpheus-research/research/MVP.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m detection_result \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39mdetect(image)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. mediapipe.python._framework_bindings.image.Image(image_format: mediapipe::ImageFormat_Format, data: numpy.ndarray[numpy.uint8])\n    2. mediapipe.python._framework_bindings.image.Image(image_format: mediapipe::ImageFormat_Format, data: numpy.ndarray[numpy.uint16])\n    3. mediapipe.python._framework_bindings.image.Image(image_format: mediapipe::ImageFormat_Format, data: numpy.ndarray[numpy.float32])\n\nInvoked with: kwargs: image_format=<ImageFormat.SRGB: 1>, data=None"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(1) # 1 may be replaced to 0 if OS is not macos\n",
    "\n",
    "while True:\n",
    "  # Reads webcam image \n",
    "  success, img = cap.read()\n",
    "\n",
    "  # Convert it to mediapipe format\n",
    "  image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "\n",
    "  # Detect face landmarks from the input image.\n",
    "  detection_result = detector.detect(image)\n",
    "\n",
    "  # Process the detection result. In this case, visualize it.\n",
    "  annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "\n",
    "  # Flip image on vertical axis for more natural effect\n",
    "  annotated_image = cv2.flip(annotated_image, 1)\n",
    "\n",
    "  # Shows transformed image on a new window\n",
    "  cv2.imshow(\"Image\", annotated_image)\n",
    "\n",
    "  # Check if a key is pressed on keyboard\n",
    "  keypress = cv2.waitKey(1)\n",
    "\n",
    "  # if key is s (save)\n",
    "  if keypress == ord('s'):\n",
    "    # Save current image in user defined folder:\n",
    "    save_image = Image.fromarray(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    resized_image = save_image.resize((SAVED_IMAGE_PIXELS, SAVED_IMAGE_PIXELS), Image.LANCZOS)\n",
    "    resized_image.save(f\"{SAVE_DIR}/img_{len(listdir(SAVE_DIR))}.png\")\n",
    "\n",
    "  # if key is q (quit)\n",
    "  elif keypress == ord('q'):\n",
    "    # Destroy all windows and release input, then quit loop\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Plot face blendshapes bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Test on static Image (not Video input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'face.png'\n",
    "\n",
    "img = cv2.imread(IMAGE_PATH) \n",
    "image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "detection_result = detector.detect(image)\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "while True:\n",
    "  cv2.imshow(\"Image\", annotated_image)\n",
    "  cv2.waitKey(0) # waitKey(0) wait until user input (keypress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
